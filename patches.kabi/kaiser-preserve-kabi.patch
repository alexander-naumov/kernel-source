From: Jiri Kosina <jkosina@suse.cz>
Subject: [PATCH] kaiser: work around kABI
Patch-mainline: Never, SUSE-specific

The most potentially dangerous one is the vmstats one.  I can't imagine what
3rd party module would realistically be directly allocating pglist_data,
per_cpu_nodestat, memcg_stat_item, lruvec_stat, etc, but the potential
non-zero risk is there.

Signed-off-by: Jiri Kosina <jkosina@suse.cz>

--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -319,7 +319,11 @@
 
 } ____cacheline_aligned;
 
+#ifndef __GENKSYMS__
 DECLARE_PER_CPU_SHARED_ALIGNED_USER_MAPPED(struct tss_struct, cpu_tss);
+#else
+DECLARE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss);
+#endif
 
 #ifdef CONFIG_X86_32
 DECLARE_PER_CPU(unsigned long, cpu_current_top_of_stack);
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -95,7 +95,11 @@
 
 static const struct cpu_dev *this_cpu = &default_cpu;
 
+#ifndef __GENKSYMS__
 DEFINE_PER_CPU_PAGE_ALIGNED_USER_MAPPED(struct gdt_page, gdt_page) = { .gdt = {
+#else
+DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
+#endif
 #ifdef CONFIG_X86_64
 	/*
 	 * We need valid kernel segments for data and code in long mode too
--- a/include/linux/mmu_context.h
+++ b/include/linux/mmu_context.h
@@ -1,7 +1,9 @@
 #ifndef _LINUX_MMU_CONTEXT_H
 #define _LINUX_MMU_CONTEXT_H
 
+#ifndef __GENKSYMS__
 #include <asm/mmu_context.h>
+#endif
 
 struct mm_struct;
 
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -39,7 +39,11 @@
  * section. Since TSS's are completely CPU-local, we want them
  * on exact cacheline boundaries, to eliminate cacheline ping-pong.
  */
+#if defined(CONFIG_GENKSYMS)
+__visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss) = {
+#else
 __visible DEFINE_PER_CPU_SHARED_ALIGNED_USER_MAPPED(struct tss_struct, cpu_tss) = {
+#endif
 	.x86_tss = {
 		.sp0 = TOP_OF_INIT_STACK,
 #ifdef CONFIG_X86_32
--- a/arch/x86/include/asm/desc.h
+++ b/arch/x86/include/asm/desc.h
@@ -43,7 +43,11 @@
 	struct desc_struct gdt[GDT_ENTRIES];
 } __attribute__((aligned(PAGE_SIZE)));
 
+#ifdef __GENKSYMS__
+DECLARE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page);
+#else
 DECLARE_PER_CPU_PAGE_ALIGNED_USER_MAPPED(struct gdt_page, gdt_page);
+#endif
 
 static inline struct desc_struct *get_cpu_gdt_table(unsigned int cpu)
 {
--- a/arch/x86/include/asm/tlbflush.h
+++ b/arch/x86/include/asm/tlbflush.h
@@ -6,7 +6,9 @@
 
 #include <asm/processor.h>
 #include <asm/special_insns.h>
+#ifndef __GENKSYMS__
 #include <asm/smp.h>
+#endif
 
 static inline void __invpcid(unsigned long pcid, unsigned long addr,
 			     unsigned long type)
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -32,7 +32,11 @@
 #include <linux/init.h>
 #include <linux/uaccess.h>
 #include <linux/highmem.h>
+#ifndef __GENKSYMS__
 #include <linux/mmu_context.h>
+#else
+#include <asm/mmu_context.h>
+#endif
 #include <linux/interrupt.h>
 #include <linux/capability.h>
 #include <linux/completion.h>
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -129,7 +129,6 @@
 	NR_PAGETABLE,		/* used for pagetables */
 	/* Second 128 byte cacheline */
 	NR_KERNEL_STACK,
-	NR_KAISERTABLE,
 	NR_UNSTABLE_NFS,	/* NFS unstable pages */
 	NR_BOUNCE,
 	NR_VMSCAN_WRITE,
@@ -154,6 +153,9 @@
 	WORKINGSET_NODERECLAIM,
 	NR_ANON_TRANSPARENT_HUGEPAGES,
 	NR_FREE_CMA_PAGES,
+#ifndef __GENKSYMS__
+	NR_KAISERTABLE,
+#endif
 	NR_VM_ZONE_STAT_ITEMS };
 
 /*
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@ -713,7 +713,6 @@
 	"nr_slab_unreclaimable",
 	"nr_page_table_pages",
 	"nr_kernel_stack",
-	"nr_overhead",
 	"nr_unstable",
 	"nr_bounce",
 	"nr_vmscan_write",
@@ -739,6 +738,7 @@
 	"workingset_nodereclaim",
 	"nr_anon_transparent_hugepages",
 	"nr_free_cma",
+	"nr_overhead",
 
 	/* enum writeback_stat_item counters */
 	"nr_dirty_threshold",
